{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c62958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\AppData\\Local\\Temp\\ipykernel_24008\\3053833495.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import csv\n",
    "import requests\n",
    "from collections import deque\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801e6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the API key from a file and remove any leading/trailing whitespace.\n",
    "with open('./API_Keys/NOAA_Token.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Base URL for the API calls.\n",
    "url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/'\n",
    "\n",
    "# Headers required for the API call, including the authorization token.\n",
    "headers = {\n",
    "    'token': api_key  # API key is passed as a token in the header.\n",
    "}\n",
    "\n",
    "def api_call(url, endpoint, headers, parameters):\n",
    "    \"\"\"\n",
    "    Make an API call to the specified URL and endpoint with given headers and parameters.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The base URL for the API.\n",
    "        endpoint (str): The specific endpoint to access data from the API.\n",
    "        headers (dict): Headers to include in the request (e.g., authorization tokens).\n",
    "        parameters (dict): Query parameters to customize the request.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The JSON response from the API if the call is successful, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(parameters['offset'])  # Debugging: print the current offset before making the call.\n",
    "        response = requests.get(url + endpoint, headers=headers, params=parameters)  # Perform the GET request.\n",
    "        response.raise_for_status()  # Check for HTTP errors and raise exceptions for them.\n",
    "        print(\"API Called\")  # Debugging: confirm the API was called.\n",
    "        return response.json()  # Return the parsed JSON response.\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API call failed: {e}\")  # Handle exceptions (e.g., network issues, 4xx and 5xx errors).\n",
    "        return None\n",
    "\n",
    "def append_to_csv(file_path, data, fieldnames):\n",
    "    \"\"\"\n",
    "    Append data to a CSV file. If the file doesn't exist, create it and write the headers.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        data (list): A list of dictionaries representing the rows to append.\n",
    "        fieldnames (list): Headers or fieldnames for the CSV.\n",
    "    \"\"\"\n",
    "    with open(file_path, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        if file.tell() == 0:  # Check if the file is empty to decide if headers need to be written.\n",
    "            writer.writeheader()\n",
    "        writer.writerows(data)  # Append the data rows to the CSV file.\n",
    "\n",
    "def append_metadata_to_csv(metadata_file_path, metadata):\n",
    "    \"\"\"\n",
    "    Append metadata to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        metadata_file_path (str): Path to the CSV file where metadata is stored.\n",
    "        metadata (dict): Metadata to be appended.\n",
    "    \"\"\"\n",
    "    with open(metadata_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metadata['resultset'].keys())\n",
    "        if file.tell() == 0:  # Check if the file is empty to decide if headers need to be written.\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metadata['resultset'])  # Append metadata to the metadata CSV.\n",
    "        \n",
    "def execute_api_calls_with_retries(base_url, endpoint, headers, initial_parameters, file_path, metadata_file_path, error_log_path, max_retries=3):\n",
    "    \"\"\"\n",
    "    Execute API calls with a mechanism for retries on failure, appending successful results to a CSV file\n",
    "    and metadata to another CSV file.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL for the API calls.\n",
    "        endpoint (str): The specific endpoint to access data from the API.\n",
    "        headers (dict): Headers to include in the request.\n",
    "        initial_parameters (dict): Initial query parameters for the API call.\n",
    "        file_path (str): Path to the CSV file where results are stored.\n",
    "        metadata_file_path (str): Path to the CSV file where metadata is stored.\n",
    "        error_log_path (str): Path to the CSV file where failed attempts are logged.\n",
    "        max_retries (int): Maximum number of retry attempts for a failed API call.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"CSV file {file_path} already exists. Operation cancelled to prevent overwriting.\")\n",
    "        return  # Prevents overwriting existing data by aborting if the file already exists.\n",
    "    \n",
    "    stack = deque([initial_parameters])  # Use a stack to manage API call parameters, starting with the initial parameters.\n",
    "    failed_attempts = []  # Track parameters that fail to succeed after the maximum number of retries.\n",
    "    \n",
    "    while stack:\n",
    "        parameters = stack.pop()  # Pop the last set of parameters to make an API call.\n",
    "        retries = parameters.pop('retries', 0)  # Extract or initialize the retry counter for these parameters.\n",
    "        data = api_call(base_url, endpoint, headers, parameters)  # Attempt the API call.\n",
    "        \n",
    "        if data and 'results' in data:  # Check if the call was successful and data was returned.\n",
    "            append_to_csv(file_path, data['results'], data['results'][0].keys())  # Append successful results to the CSV.\n",
    "            append_metadata_to_csv(metadata_file_path, data['metadata'])  # Append metadata to the metadata CSV\n",
    "            \n",
    "            # Check if there are more results to fetch and prepare the next set of parameters.\n",
    "            if parameters.get('offset', 1) + parameters.get('limit', 10) - 1 < data['metadata']['resultset']['count']:\n",
    "                next_params = parameters.copy()\n",
    "                next_params['offset'] += parameters.get('limit', 10)\n",
    "                stack.append(next_params)  # Add the next parameters to the stack for subsequent API calls.\n",
    "        else:\n",
    "            # Retry logic for failed attempts.\n",
    "            if retries < max_retries:\n",
    "                parameters['retries'] = retries + 1  # Increment the retry counter.\n",
    "                stack.append(parameters)  # Re-add the parameters to the stack for retry.\n",
    "            else:\n",
    "                failed_attempts.append(parameters)  # Log parameters that exceeded max retries.\n",
    "                \n",
    "    # After all attempts, log any parameters that failed to succeed after max retries to a file.\n",
    "    if failed_attempts:\n",
    "        with open(error_log_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(failed_attempts)  # Log failed attempts for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919a9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file all_stations.csv already exists. Operation cancelled to prevent overwriting.\n"
     ]
    }
   ],
   "source": [
    "# Example of how to call the function\n",
    "endpoint = \"/stations/\"\n",
    "parameters = {\n",
    "    \"locationid\": \"FIPS:27\",\n",
    "    \"limit\": 1000,\n",
    "    \"offset\": 1\n",
    "}\n",
    "file_path = \"all_stations.csv\"\n",
    "error_log_path = \"error_log.csv\"\n",
    "metadata_path = \"stations_meta.csv\"\n",
    "\n",
    "execute_api_calls_with_retries(url, endpoint, headers, parameters, file_path, metadata_path, error_log_path, max_retries=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e1ea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "API Called\n",
      "CSV file all_TMAX.csv already exists. Operation cancelled to prevent overwriting.\n"
     ]
    }
   ],
   "source": [
    "#Test downloading data from all stations\n",
    "endpoint = '/data/'\n",
    "\n",
    "# Specify the dataset, station ID, and date range\n",
    "parameters = {\n",
    "    'datasetid': 'GHCND',\n",
    "    'locationid': 'FIPS:27',\n",
    "    'datatypeid': 'TMAX',\n",
    "    'limit': 1000, # max is 1000 for a single request\n",
    "    'offset': 1,\n",
    "    'units': 'metric',\n",
    "    'startdate': '2023-01-01',\n",
    "    'enddate': '2023-12-31'\n",
    "}\n",
    "\n",
    "data = api_call(url, endpoint, headers, parameters)\n",
    "\n",
    "file_path = \"all_TMAX.csv\"\n",
    "error_log_path = \"error_log.csv\"\n",
    "metadata_path = \"stations_meta.csv\"\n",
    "\n",
    "execute_api_calls_with_retries(url, endpoint, headers, parameters, file_path, metadata_path, error_log_path, max_retries=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6e5975-1023-4f39-aecd-844883511d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "API Called\n",
      "CSV file all_TMIN.csv already exists. Operation cancelled to prevent overwriting.\n"
     ]
    }
   ],
   "source": [
    "#Test downloading data from all stations\n",
    "endpoint = '/data/'\n",
    "\n",
    "# Specify the dataset, station ID, and date range\n",
    "parameters = {\n",
    "    'datasetid': 'GHCND',\n",
    "    'locationid': 'FIPS:27',\n",
    "    'datatypeid': 'TMIN',\n",
    "    'limit': 1000, # max is 1000 for a single request\n",
    "    'offset': 1,\n",
    "    'units': 'metric',\n",
    "    'startdate': '2023-01-01',\n",
    "    'enddate': '2023-12-31'\n",
    "}\n",
    "\n",
    "data = api_call(url, endpoint, headers, parameters)\n",
    "\n",
    "file_path = \"all_TMIN.csv\"\n",
    "error_log_path = \"error_log.csv\"\n",
    "metadata_path = \"stations_meta.csv\"\n",
    "\n",
    "execute_api_calls_with_retries(url, endpoint, headers, parameters, file_path, metadata_path, error_log_path, max_retries=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6eb0255-426a-476b-837b-7abc06ea3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data CLEANING\n",
    "tmin_df = pd.read_csv('ALL_TMIN.csv')\n",
    "tmax_df = pd.read_csv('ALL_TMAX.csv')\n",
    "\n",
    "# Merge TMAX and TMIN on 'date' and 'station'\n",
    "merged_temp_df = pd.merge(tmax_df[['date', 'station', 'value']], tmin_df[['date', 'station', 'value']], on=['date', 'station'], suffixes=('_TMAX', '_TMIN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ea2515-28ab-4603-91e1-0c09e3da45fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station</th>\n",
       "      <th>value_TMAX</th>\n",
       "      <th>value_TMIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01T00:00:00</td>\n",
       "      <td>GHCND:CA006020559</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01T00:00:00</td>\n",
       "      <td>GHCND:USC00210018</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01T00:00:00</td>\n",
       "      <td>GHCND:USC00210050</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>-8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01T00:00:00</td>\n",
       "      <td>GHCND:USC00210075</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01T00:00:00</td>\n",
       "      <td>GHCND:USC00210190</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63585</th>\n",
       "      <td>2023-12-31T00:00:00</td>\n",
       "      <td>GHCND:USW00094961</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63586</th>\n",
       "      <td>2023-12-31T00:00:00</td>\n",
       "      <td>GHCND:USW00094963</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63587</th>\n",
       "      <td>2023-12-31T00:00:00</td>\n",
       "      <td>GHCND:USW00094967</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>-7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63588</th>\n",
       "      <td>2023-12-31T00:00:00</td>\n",
       "      <td>GHCND:USW00094976</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>-7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63589</th>\n",
       "      <td>2023-12-31T00:00:00</td>\n",
       "      <td>GHCND:USW00094992</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>-8.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63590 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date            station  value_TMAX  value_TMIN\n",
       "0      2023-01-01T00:00:00  GHCND:CA006020559        -4.0        -6.0\n",
       "1      2023-01-01T00:00:00  GHCND:USC00210018        -4.4        -9.4\n",
       "2      2023-01-01T00:00:00  GHCND:USC00210050        -5.6        -8.3\n",
       "3      2023-01-01T00:00:00  GHCND:USC00210075         1.1        -5.0\n",
       "4      2023-01-01T00:00:00  GHCND:USC00210190         1.7        -6.1\n",
       "...                    ...                ...         ...         ...\n",
       "63585  2023-12-31T00:00:00  GHCND:USW00094961        -3.8        -7.7\n",
       "63586  2023-12-31T00:00:00  GHCND:USW00094963        -1.6        -4.9\n",
       "63587  2023-12-31T00:00:00  GHCND:USW00094967        -5.5        -7.7\n",
       "63588  2023-12-31T00:00:00  GHCND:USW00094976        -4.3        -7.7\n",
       "63589  2023-12-31T00:00:00  GHCND:USW00094992        -3.2        -8.8\n",
       "\n",
       "[63590 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67485fbd-1f96-4581-befd-12b6b115a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_temp_df.to_csv('TMIN_TMAX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ac40b9-93e7-4dff-b5c2-d498b7fca985",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_tmax_df = pd.read_csv('TMIN_TMAX.csv')\n",
    "stations_df = pd.read_csv('all_stations.csv')\n",
    "\n",
    "# Perform the merge again without extracting the station ID, using the full 'station' value from TMIN_TMAX DataFrame\n",
    "# This time, we directly compare the 'station' field in TMIN_TMAX with the 'id' field in the Stations DataFrame\n",
    "merged_df_correct = pd.merge(tmin_tmax_df, stations_df, left_on='station', right_on='id', how='left')\n",
    "\n",
    "# Select relevant columns, including latitude and longitude from the Stations DataFrame\n",
    "merged_df = merged_df_correct[['date', 'station', 'value_TMAX', 'value_TMIN', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4389137-9bf7-4353-a242-684af077dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('temp_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8844c90c-5ae9-47eb-bdf0-81bd68b6c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Check:\n",
      " value_TMAX    0\n",
      "value_TMIN    0\n",
      "dtype: int64\n",
      "\n",
      "Logical Inconsistency Check:\n",
      " 0\n",
      "\n",
      "Temperature Outliers Check:\n",
      " {'TMAX_below_typical_min': 0, 'TMAX_above_typical_max': 0, 'TMIN_below_typical_min': 0, 'TMIN_above_typical_max': 0}\n",
      "\n",
      "Duplicate Dates Check:\n",
      " 0\n",
      "\n",
      "Potential Gaps in Data Check:\n",
      " 753\n",
      "\n",
      "Cross-Station Temperature Consistency:\n",
      "         date  TMAX_mean  TMAX_std  TMIN_mean  TMIN_std\n",
      "0 2023-01-01  -0.919209  2.664598  -8.201695  2.973624\n",
      "1 2023-01-02  -3.122222  3.249058  -9.074444  3.622973\n",
      "2 2023-01-03  -4.654696  3.334490 -10.623757  4.870494\n",
      "3 2023-01-04  -3.496154  3.498735  -8.854396  5.491335\n",
      "4 2023-01-05  -4.180110  3.579858 -12.696685  6.514513\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('temp_data.csv')\n",
    "\n",
    "# Convert date column to datetime for easier manipulation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 1. Missing Values Check\n",
    "missing_values_check = df[['value_TMAX', 'value_TMIN']].isnull().sum()\n",
    "\n",
    "# 2. Logical Consistency Check (TMIN should not be greater than TMAX)\n",
    "logical_inconsistency_check = df[df['value_TMIN'] > df['value_TMAX']].shape[0]\n",
    "\n",
    "# 3. Temperature Range Outliers Check\n",
    "# Assuming typical extreme temperature ranges for Minnesota: -51°C to 46°C\n",
    "temperature_outliers_check = {\n",
    "    'TMAX_below_typical_min': df[df['value_TMAX'] < -51].shape[0],\n",
    "    'TMAX_above_typical_max': df[df['value_TMAX'] > 46].shape[0],\n",
    "    'TMIN_below_typical_min': df[df['value_TMIN'] < -51].shape[0],\n",
    "    'TMIN_above_typical_max': df[df['value_TMIN'] > 46].shape[0]\n",
    "}\n",
    "\n",
    "# 4. Duplicate Dates Check\n",
    "duplicate_dates_check = df.duplicated(subset=['station', 'date']).sum()\n",
    "\n",
    "# 5. Potential Gaps in Data\n",
    "df_sorted = df.sort_values(by=['station', 'date'])\n",
    "df_sorted['next_date'] = df_sorted.groupby('station')['date'].shift(-1)\n",
    "df_sorted['date_diff'] = df_sorted['next_date'] - df_sorted['date']\n",
    "gaps_check = df_sorted[df_sorted['date_diff'] > timedelta(days=1)].shape[0]\n",
    "\n",
    "# 6. Cross-Station Temperature Consistency\n",
    "temperature_variance = df.groupby('date').agg({\n",
    "    'value_TMAX': ['mean', 'std'],\n",
    "    'value_TMIN': ['mean', 'std']\n",
    "}).reset_index()\n",
    "temperature_variance.columns = ['date', 'TMAX_mean', 'TMAX_std', 'TMIN_mean', 'TMIN_std']\n",
    "\n",
    "# Print results (or further analyze, depending on your needs)\n",
    "print(\"Missing Values Check:\\n\", missing_values_check)\n",
    "print(\"\\nLogical Inconsistency Check:\\n\", logical_inconsistency_check)\n",
    "print(\"\\nTemperature Outliers Check:\\n\", temperature_outliers_check)\n",
    "print(\"\\nDuplicate Dates Check:\\n\", duplicate_dates_check)\n",
    "print(\"\\nPotential Gaps in Data Check:\\n\", gaps_check)\n",
    "print(\"\\nCross-Station Temperature Consistency:\\n\", temperature_variance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c99d92-0579-451a-9913-df3bc4021450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
